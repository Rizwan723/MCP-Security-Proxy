# ==============================================================================
# MCP Security Gateway - Environment Configuration
# ==============================================================================
# Copy this file to .env and configure for your environment
# Never commit .env to version control!

# ==============================================================================
# MCP BRIDGE (Security Proxy) Configuration
# ==============================================================================

# Logging
LOG_LEVEL=INFO
DEBUG=false

# Scientific Parameters
MODEL_NAME=distilbert-base-uncased
DETECTOR_SIGMA=3.0

# Service Discovery - MCP Tool URLs
TOOL_FS_URL=http://tool-filesystem:8080
TOOL_SQL_URL=http://tool-sqlite:8080
TOOL_TIME_URL=http://tool-time:8080

# Model Persistence Paths (Container paths)
RESEARCH_DATA_PATH=/app/research_data
TRAINING_DATA_FILE=training_dataset.json
SEMANTIC_MODEL_PATH=/app/trained_models/semantic_model.pt
STATISTICAL_MODEL_PATH=/app/trained_models/statistical_model.pt
AUDIT_LOG_FILE=runtime_audit.jsonl

# ==============================================================================
# LOCAL LLM SERVICE Configuration
# ==============================================================================

# Model Selection (choose one of the available models)
# Available models in ./models/:
# - llama-2-7b-chat.Q4_K_M.gguf (Fast, lower memory)
# - llama-2-7b-chat.Q8_0.gguf (Better quality)
# - Meta-Llama-3-8B-Instruct.Q5_K_M.gguf
# - Mistral-7B-Instruct-v0.3.Q5_K_M.gguf
# - Mistral-7B-Instruct-v0.3.Q8_0.gguf
LLM_MODEL_PATH=/app/models/llama-2-7b-chat.Q4_K_M.gguf

# Model Parameters
LLM_N_CTX=4096
LLM_N_BATCH=512
LLM_N_GPU_LAYERS=-1
LLM_FLASH_ATTN=false
LLM_VERBOSE=true
LLM_CHAT_FORMAT=llama-2

# Generation Defaults
LLM_TEMPERATURE=0.7
LLM_MAX_TOKENS=512

# ==============================================================================
# CLOUD LLM SERVICE Configuration
# ==============================================================================

# Provider: 'openai' or 'gemini'
LLM_PROVIDER=gemini

# API Keys (required for cloud providers)
OPENAI_API_KEY=your-openai-api-key-here
GOOGLE_API_KEY=your-google-api-key-here

# Model Selection
# OpenAI: gpt-3.5-turbo, gpt-4, gpt-4-turbo
# Gemini: gemini-pro, gemini-1.5-pro
CLOUD_MODEL_NAME=gemini-pro

# Generation Parameters
CLOUD_TEMPERATURE=0.7
CLOUD_MAX_TOKENS=1024

# ==============================================================================
# MCP TOOLS Configuration
# ==============================================================================

# Filesystem Tool
SAFE_MODE=false
FS_ROOT_DIR=/data

# SQLite Tool
SQL_DB_PATH=thesis.db

# ==============================================================================
# CLIENT Configuration
# ==============================================================================

# Bridge Endpoint
BRIDGE_RPC_URL=http://mcp-bridge:8000/jsonrpc

# LLM Service URLs
CUSTOM_LLM_URL=http://custom-llm:8080/v1
LLM_CLOUD_URL=http://cloud-llm:8080/v1

# Traffic Generator Settings
TRAFFIC_SAMPLES=50
TRAFFIC_ATTACK_RATIO=0.3
TRAFFIC_CONCURRENCY=5

# ==============================================================================
# DOCKER & NETWORKING
# ==============================================================================

# Port Mappings (Host:Container)
# Default values match docker compose.yml
CUSTOM_LLM_PORT=8080
LLM_CLOUD_PORT=8081
BRIDGE_PORT=8000

# ==============================================================================
# RESEARCH & DEVELOPMENT
# ==============================================================================

# Enable experimental features
ENABLE_EXPERIMENTAL=false

# Save audit logs for analysis
SAVE_AUDIT_LOGS=true

# Training dataset paths (container paths)
TRAINING_DATASET_PATH=/app/research_data/training_dataset.json
TEST_DATASET_PATH=/app/research_data/test_dataset.json
